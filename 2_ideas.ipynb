{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2898606",
   "metadata": {},
   "source": [
    "- Small-World + Modular Netzwerk hinzufuegen, das beides vereint\n",
    "\n",
    "- Evaluationsmetriken erstmal auslesen\n",
    "\n",
    "- Modular mit mehr communities testen\n",
    "\n",
    "- More Layers\n",
    "\n",
    "- RNNs\n",
    "\n",
    "\n",
    "\n",
    "Questions that need Clarification:\n",
    "- Why do Episode lengths differ between topologies?\n",
    "- What kind of baseline does Fully Connected provide? Does it have inherent advantages because of more connections or even more parameters?\n",
    "- Influence of where and which are input and output?\n",
    "- What role does the number of hidden nodes play?\n",
    "- What does input stubs mean?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95224957",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- understand exact NEAT generation process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa4543",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9524e0a7",
   "metadata": {},
   "source": [
    "### 1   Big picture \n",
    "| Phase                          | Core research goal                                                         | Concrete deliverables you’ve listed                                                                                                                                                                                                                                                   | Extra choices you’ll need to lock-in                                                                                                                                                                  |\n",
    "| ------------------------------ | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **1. Topology ⇢ RL fitness**   | Quantify how structural **form** affects one-shot RL performance           | – Implement FC, random-sparse, Watts-Strogatz SW, explicit modular, SW + modular (NEAT or direct generators)  <br>– Compute structural metrics (ρ, C, L, σ, Q, etc.)  <br>– Reward curves, sample-efficiency, parameter count  <br>– Correlation / ANOVA linking structure ↔ RL score | • Which RL tasks? (simple control vs. Atari vs. MuJoCo)  <br>• Which RL algorithm family (value-based, actor-critic, evolutionary)?  <br>• How many random seeds for significance?                    |\n",
    "| **2. Continual / Lifelong**    | Identify structures that **retain & transfer** across a task sequence      | – Define task curriculum (e.g., Continual-World, MiniGrid-LevelGen, ProcGen variations)  <br>– Metrics:  **Forward transfer**, **Average forgetting**, final **area under reward curve**  <br>– Compare topologies from Phase 1 under identical curricula                             | • Curriculum length & similarity gradient  <br>• Whether to freeze weights vs. fine-tune  <br>• Use regularisation baselines (EWC, SI, L2) as controls                                                |\n",
    "| **3. NDP-controlled rigidity** | Let a **Neural Developmental Program** decide what stays rigid vs. plastic | – Implement/core-use Nisioti-style NDP that outputs connection graph **and** a “rigidity gate” per edge/neuron  <br>– Train NDP over the same curricula; measure emergent rigidity patterns  <br>– Analyse correlations between learned rigidity and metrics from Phases 1–2          | • Encoding of rigidity (scalar temperature? binary mask? synaptic consolidation coefficient)  <br>• How often NDP can mutate structure during life  <br>• Compute cost of plasticity vs. frozen parts |\n",
    "\n",
    "\n",
    "\n",
    "### 2 Experimental stack\n",
    "\n",
    "| Layer                  | Recommended option                                                                                                                                                                                                                                             | Rationale                                                                                                             |\n",
    "| ---------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| **RL environments**    | **Start small**: CartPole, MountainCarContinuous, LunarLander, then **scale** to *MiniGrid LevelGen*, *Continual-World (HalfCheetah angles)*, or *ProcGen* variants.                                                                                           | Cheap early iterations; later tasks give you continuous control and vision-based curricula.                           |\n",
    "| **RL algorithms**      | • PPO (stable, on-policy)<br>• A2C (fast baseline)<br>• ES/NEAT (for graph-evolving baselines)                                                                                                                                                                 | Gives you gradient-based + evolutionary flavours, and both can share the same graph substrate.                        |\n",
    "| **Graph generators**   | • **FC**: trivial dense adjacency<br>• **Random-sparse**: Erdős–Rényi *(N, p)*<br>• **Modular**: Stochastic Block Model *(p\\_intra ≫ p\\_inter)*<br>• **SW**: Watts-Strogatz *(k, β)*<br>• **SW+Mod**: SBMs with WS inside blocks + random long-range shortcuts | All have two or three tunable knobs you can sweep while “staying inside” the topology class (as we detailed earlier). |\n",
    "| **Structural metrics** | ρ, C, L, σ, Q, assortativity, average degree, weighted efficiency                                                                                                                                                                    | Store per-run; you’ll need them for correlation & ANOVA.                                                              |\n",
    "\n",
    "\n",
    "\n",
    "### 3 Statistics & analysis blueprint\n",
    "\n",
    "1. **Phase 1 cross-section:**\n",
    "   *Two-way ANOVA* with factors **Topology** × **Task** on (a) final episodic reward, (b) AUC of learning curve, (c) parameter count.\n",
    "   Follow with **Pearson/Spearman correlation** between each structural metric and each performance metric.\n",
    "\n",
    "2. **Phase 2 continual:**\n",
    "   Compute for every task *t* in the curriculum:\n",
    "\n",
    "   $$\n",
    "   \\text{Forgetting}(t)=\\max_{k\\le t} R_k - R_t\n",
    "   $$\n",
    "\n",
    "   then report **Average Forgetting**, **Forward Transfer** (first-episode reward on task *t+1*), and **Backward Transfer**.\n",
    "   Same ANOVA structure but substitute *Topology* × *Curriculum*.\n",
    "\n",
    "3. **Phase 3 rigidity patterns:**\n",
    "   *Cluster* learned rigidity coefficients and relate cluster membership to (a) module boundaries, (b) node centrality, (c) structural metric values.\n",
    "   Possible tools: Mantel test (matrix correlation between rigidity matrix and adjacency), mutual information between rigidity mask and community labels.\n",
    "\n",
    "\n",
    "### 4 Open design questions (worth nailing down early)\n",
    "\n",
    "| Question                                  | Why it matters                                                                        | Typical choices                                                                      |\n",
    "| ----------------------------------------- | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |\n",
    "| **What counts as “good” RL performance?** | Sample-efficiency vs. asymptotic vs. compute budget can lead to opposite conclusions. | Use both AUC (early learning) and final reward; normalise by FLOPs or wall-clock.    |\n",
    "| **Weight-sharing vs. per-edge weights?**  | Graphs with repeated edge weights behave differently (esp. modular).                  | Probably unique weights; but you can test hash-based parameter tying as an ablation. |\n",
    "| **Plasticity rule during lifetime?**      | Hebbian, Oja, or gradient? For NDP you might inject online Hebbian updates.           | Start with SGD/Adam only; later add Hebbian channels if time allows.                 |\n",
    "| **Plasticity cost**                       | Free plasticity biases toward fully plastic networks.                                 | Impose an L1 penalty on non-rigid gates or limit number of plastic edges.            |\n",
    "| **Computation budget**                    | Determines number of random seeds × tasks × topologies you can afford.                | Rough planning: 5 seeds × 5 topologies × 4 tasks ≈ 100 training runs per phase.      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9cc44a",
   "metadata": {},
   "source": [
    "- Trying to extract the effect of weight vs. the degree of the node\n",
    "    - Apparently the others already tried to see if there is a difference in weight updates, depending on topology... In FC networks weight updates are quite similar throughout the network\n",
    "- Metric about effect weights in non-fully connected networks "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
