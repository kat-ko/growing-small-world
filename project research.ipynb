{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ef8bec",
   "metadata": {},
   "source": [
    "## What does it mean to be a good network topology for RL?\n",
    "- Evaluate how various network topologies affect RL performance on standard tasks\n",
    "- Which topologies (small-world, fully-connected, random sparse, modular) provide superior performance in standard RL tasks?\n",
    "- What structural properties (e.g., clustering, sparsity, path length) correlate with improved RL performance?\n",
    "\n",
    "Desired outcome:\n",
    "- Empirical understanding of which structural properties correlate with good RL performance\n",
    "a. Implement and evaluate standard architectures (fully-connected, random sparse, small-world via NEAT)\n",
    "b. Analyze metrics: clustering, sparsity, path length.\n",
    "c. Quantitative analysis (like correlation & ANOVA) to identify beneficial structural properties.\n",
    "\n",
    "Remarks:\n",
    "- Which tasks to use for this test? \n",
    "    - Parity Task\n",
    "- Implement different network topologies\n",
    "    - Fully-Connected\n",
    "    - Small-World grown with NEAT\n",
    "    - Random-sparse\n",
    "    - What else could be tested?\n",
    "    --> Which metrics? p and k \n",
    "- What other structural properties can be interesting?\n",
    "    - what means clustering, sparsity, path length? run subtests on these?\n",
    "-  Interplay with modularity? Which role does structural modularity play vs. is there a functional modularity (or specialization)?\n",
    "\n",
    "\n",
    "\n",
    "- Train each network on standard benchmarks (e.g., CartPole, LunarLander, GridWorld).\n",
    "- Extract topology stats after training (or during, if topology changes dynamically).\n",
    "- Run analysis to find which structural properties correlate with high performance.\n",
    "\n",
    "\n",
    "\n",
    "Security Aspects:\n",
    "- Avoid layers that could reintroduce full connectivity (e.g., LayerNorm, residuals that shortcut through topology).\n",
    "- Ensure no automatic pruning or rewiring during training unless intended.\n",
    "- Monitor adjacency matrices (or weight masks) during training to verify structure preservation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674325b",
   "metadata": {},
   "source": [
    "## How should network structures look to be easily adapted by RL for lifelong learning?\n",
    "- Determine what structures can be effectively adapted by RL across tasks\n",
    "- What are the topological features that support adaptation across different tasks in a lifelong learning scenario?\n",
    "- How does network structure impact robustness against catastrophic forgetting?\n",
    "\n",
    "Desired Outcome:\n",
    "Identification of network properties conducive to lifelong learning & Understanding trade-offs between adaptability and forgetting\n",
    "a. Decide on a controlled task distribution (idea: keperas)\n",
    "b. Incrementally train networks on task sequences, measure how efficiently they adapt\n",
    "c. Compare the structures identified in Phase 1 for their lifelong learning capability\n",
    "d. Explicitly assess catastrophic forgetting\n",
    "\n",
    "Tasks:\n",
    "- How to test for lifelong learning capabilities?\n",
    "- How to test for catastrophic forgetting?\n",
    "- Which role does scale play (# of parameters)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a84f5b",
   "metadata": {},
   "source": [
    "## How can an NDP learn to differentiate flexible vs. rigid structures in neural networks?\n",
    "- Can an NDP learn which parts of a network should remain rigid or become flexible over extended task distributions?\n",
    "- How does this affect the network’s lifelong learning capability?\n",
    "- Which role does the layer play? --> Also relates to where Input/Output are positioned\n",
    "\n",
    "\n",
    "- Insight into how developmental processes influence structural adaptability & NDP-driven rigidity management with idea to improve lifelong learning capabilities\n",
    "    1. Implement a basic NDP architecture (check this with Nisioti et al.) that outputs not only connection structures but also rigidity/flexibility parameters per connection or neuron\n",
    "    2. Train NDP-generated networks on lifelong learning task sequences from Phase 2, allowing the NDP to control structural rigidity\n",
    "    3. Analyze emergent rigidity patterns, relating them explicitly to network performance, adaptability, and resistance to forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163c3e8",
   "metadata": {},
   "source": [
    "# Next REAL Journal Entry:\n",
    "\n",
    "\n",
    "TODO: Tackle the relation to memory and functional modularity\n",
    "\n",
    "What makes memory:\n",
    "- not like the paper, where they say attentional bias makes memory (What did Chris critique here?)\n",
    "\n",
    "Functional Modularity:\n",
    "- seems to be an emergent phenomenon, but doesn't just emerge randomly, but given a certain topology and certain pressure (sparse ressources or something like that?)\n",
    "\n",
    "\n",
    "## Future Work\n",
    "- Emergence of modularity\n",
    "- Concerning Modularity: How could functional modularity (in contrast to structural modularity) interplay with the whole project?\n",
    "- By analyzing how symmetry breaks through training, we might uncover what structural differentiations (e.g. modularization, edge pruning) emerge to support specialized functions.\n",
    "    --> Found your \"Structurally Flexible Neural Networks: Evolving the Building Blocks for General Agents\" Paper, where topologically the network topology is sparse and sampled randomly per lifetime. And around 50% of possible connections are removed at random. This random sampling is not evolved, but acts as a symmetry-breaking mechanism. Random sparse connectivity leads to better generalization and adaptability across tasks. Fully connected networks (SFNN_fully) perform worse — they tend to oversmooth and collapse into homogeneous activations, failing to differentiate functions across units. Fixed topology during evolution (Fixed_SFNN) overfits and loses the generalization benefits of structure variation.\n",
    "    --> Online RL requires topology to be good for gradient updates to work well.\n",
    "    --> SFNN requires topology to be good for self-organization during lifetime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06c40f",
   "metadata": {},
   "source": [
    "# References: \n",
    "\n",
    "### Dynamics of specialization in neural modules under resource constraints - https://www.nature.com/articles/s41467-024-55188-9\n",
    "- \"Modularity is an enticing concept that naturally fits the way we attempt to understand and engineer complex systems. We tend to break down difficult concepts into smaller, more manageable parts. Modularity has a clear effect in terms of robustness and interpretability in such systems. Disentangled functionality means that the impairment of a module doesn’t lead to the impairment of the whole, while making it easy to spot critical failure points.\"\n",
    "- \"As a more emergent principle linking structure and function, it has been suggested that modularity emerged as a byproduct of the evolutionary pressure to reduce the metabolic costs of building, maintaining and operating neurons and synapses. It has been shown that brain networks are near-optimal when it comes to minimizing both its wiring and running metabolic costs.\" \n",
    "- \"distinguish two types of modularity, structural and functional, and understand how they are related. We take structural modularity to mean the degree to which a neural network is organized into discrete and differentiated modules. Such modules show much denser connections internally than across other modules. This is usually computed using the Q-metric, measuring how much more clustered these modules are when compared to a graph connected at random.\"\n",
    "- \"Although note that many other techniques are possible, and that module detection in networks17 as well as defining measures of modularity18,19 are complex and interesting fields in their own right.\"\n",
    "- While this structural definition is important, it doesn’t necessarily inform us on the function of the modules. \n",
    "--> But here we jump in with the influence on RL\n",
    "--> Here we don't care about the functional modularity (only if it emerges naturally?)\n",
    "- Functional modularity: \"*Separate modifiability* means that the impairment of one module should not affect the functioning of another.\"\n",
    "--> Is this what is necessary for safe mutations? \n",
    "- \"Generally, the link between structural and functional modularity is context-dependent and involves a complex and dynamic interplay of several internal and external variables. However, it is unclear the extent to which structural modularity is important for the emergence of specialization through training. We show here a case where even under strict structural modularity conditions, modules exhibit entangled functional behaviors. We then explore the space of architectures, resource constraints, and environmental structure in a more systematic way, and find sets of necessary constraints (within our constrained setup) for the emergence of specialized functions in the modules.\"\n",
    "\n",
    "Methods:\n",
    "- \"these modules consist of vanilla RNNs, but the code is written to allow the use of other modules types, such as GRUs.\"\n",
    "- \". We vary n, p, the pathway structure, and the presence of the bottleneck layer.\"\n",
    "- \"The choice of a recurrent rather than feed-forward architecture was made to keep a consistent architecture throughout the paper and to simplify the definitions of functional specialization and modularity.\"\n",
    "- Structural modularity: \"We define the fraction of connections between two modules of size n as p ∈ [1/n2, 1]. The same fraction of connections is used in each direction. The smallest value is p = 1/n2 corresponding to a single connection in each direction, and the largest fraction p = 1 corresponds to n2 connections in each direction (all-to-all connectivity).\"\n",
    "\n",
    "Results:\n",
    "-  \"in at least one simple type of network, imposing moderately high levels of structural modularity doesn’t directly lead to the emergence of specialized modules.\"\n",
    "- \"a high level of structural modularity isn’t a sufficient condition for the emergence of specialization.\"\n",
    "- \"One limitation on this conclusion is that we have relied on the well-established Q-metric from network theory. This metric is widely used in connectomics research, but may not be the best measure of structural modularity (although we did not systematically investigate alternatives).\"\n",
    "- \"other studies have taken a more emergent approach: various researchers54,55 have investigated whether modular properties can emerge directly from the first principle of minimizing connection costs. Spatially-embedded networks, regularized to minimize connection costs while learning, do end up displaying modular and small-world features, but we note that both works had to introduce an additional regularization or optimization technique to see it emerge. Understanding if, and if so how, structural and functional modularity can emerge from purely low level and naturalistic principles outside of a controlled setup thus remains an open question.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795e300",
   "metadata": {},
   "source": [
    "Difference Modularity and Small-Worldness:\n",
    "- Modularity refers to the presence of tightly connected subgroups (modules) within a network that are loosely connected to other modules. Each module might specialize in a sub-function or sub-task.\n",
    "- Small-worldness describes networks that exhibit:\n",
    "    - High clustering (like modularity) and\n",
    "    - Short average path lengths (like random graphs), which facilitates efficient communication across the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443a47f",
   "metadata": {},
   "source": [
    "Difference between continuous learning and lifelong learning\n",
    "- sometimes used interchangeably in casual contexts even though there is a difference\n",
    "- Lifelong Learning\n",
    "    - Focuses on an agent's ability to retain and reuse knowledge across tasks over a long time.\n",
    "    - Involves task boundaries (task A, then task B, etc.).\n",
    "    - Concerns: catastrophic forgetting, transfer learning, memory mechanisms.\n",
    "- Continuous Learning\n",
    "    - Emphasizes learning in a streaming, non-stationary environment.\n",
    "    - Often assumes no explicit task boundaries.\n",
    "    - Focuses on gradual adaptation, robustness to distribution shifts.\n",
    "- TL;DR: \n",
    "    - Lifelong = \"curriculum of tasks with memory\"\n",
    "    - Continuous = \"ongoing environmental change with adaptation\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
